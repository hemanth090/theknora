# Groq API Configuration
GROQ_API_KEY=your_groq_api_key_here

# Vector Store Configuration
VECTOR_STORE_PATH=data/vector_store
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Server Configuration
SERVER_HOST=127.0.0.1
SERVER_PORT=8000

# LLM Configuration
DEFAULT_LLM_MODEL=openai/gpt-oss-120b

# Logging
RUST_LOG=info
```

Now let me create the README for the backend:

<file_path>
knora_ai-main\knora-rust-nextjs\backend\README.md
</file_path>

<edit_description>
Create backend README documentation
</edit_description>

```
# KnoRa AI Backend - Rust Implementation

A high-performance Rust backend for the KnoRa AI Knowledge Assistant, providing document processing, vector search, and LLM integration capabilities.

## üöÄ Features

- **Document Processing**: Support for PDF, DOCX, CSV, XLSX, TXT, MD, PPTX files
- **Vector Search**: FAISS-based semantic search with embedding caching
- **LLM Integration**: Groq API integration with streaming support
- **Caching**: Query response and embedding caching for improved performance
- **REST API**: Comprehensive REST endpoints for all operations
- **CORS Support**: Full CORS configuration for frontend integration
- **Async/Await**: Built on Actix-web for high concurrency

## üìã Prerequisites

- Rust 1.70+ (install from https://rustup.rs/)
- Groq API key (get free key at https://console.groq.com)

## üîß Setup

### 1. Install Dependencies

```bash
# Ensure you have Rust installed
rustc --version
cargo --version
```

### 2. Configure Environment

Copy `.env.example` to `.env` and configure:

```bash
cp .env.example .env
```

Edit `.env` with your settings:

```env
GROQ_API_KEY=your_actual_groq_api_key
VECTOR_STORE_PATH=data/vector_store
SERVER_HOST=127.0.0.1
SERVER_PORT=8000
```

### 3. Build the Project

```bash
cargo build --release
```

### 4. Run the Server

```bash
cargo run --release
```

Server will start at `http://127.0.0.1:8000`

## üìö API Endpoints

### Health Check

```
GET /api/health
```

Returns server status and version information.

### Document Processing

#### Process File
```
POST /api/documents/process
Content-Type: application/json

{
  "file_path": "/path/to/document.pdf",
  "chunk_size": 1000,
  "chunk_overlap": 200
}
```

#### Get File Statistics
```
GET /api/documents/stats?file_path=/path/to/document.pdf
```

### Search

#### Search Vector Store
```
POST /api/search
Content-Type: application/json

{
  "query": "What is machine learning?",
  "k": 5,
  "score_threshold": 0.0
}
```

#### Get Vector Store Statistics
```
GET /api/search/stats
```

#### Add Documents to Vector Store
```
POST /api/search/add
Content-Type: application/json

[
  {
    "file_path": "/path/to/doc.pdf",
    "file_name": "doc.pdf",
    "file_type": ".pdf",
    "text": "Document text...",
    "chunks": [...],
    "num_chunks": 5,
    "file_size": 1024000
  }
]
```

#### Delete Document
```
DELETE /api/search/delete?file_path=/path/to/document.pdf
```

#### Clear Vector Store
```
DELETE /api/search/clear
```

### LLM

#### Generate Answer
```
POST /api/llm/answer
Content-Type: application/json

{
  "query": "What are the key findings?",
  "retrieved_chunks": [...],
  "max_tokens": 8192,
  "temperature": 1.0
}
```

#### Get Model Information
```
GET /api/llm/model-info
```

#### Get Supported Models
```
GET /api/llm/models
```

## üß™ Testing

Run tests:

```bash
cargo test
```

Run tests with logging:

```bash
RUST_LOG=debug cargo test -- --nocapture
```

## üìÅ Project Structure

```
backend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.rs                 # Application entry point
‚îÇ   ‚îú‚îÄ‚îÄ config.rs               # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ models.rs               # Data models
‚îÇ   ‚îú‚îÄ‚îÄ handlers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mod.rs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document.rs         # Document endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search.rs           # Search endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm.rs              # LLM endpoints
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ health.rs           # Health check
‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ       ‚îú‚îÄ‚îÄ mod.rs
‚îÇ       ‚îú‚îÄ‚îÄ document_processor.rs    # Document processing
‚îÇ       ‚îú‚îÄ‚îÄ vector_store.rs          # Vector database
‚îÇ       ‚îú‚îÄ‚îÄ llm_handler.rs           # LLM integration
‚îÇ       ‚îî‚îÄ‚îÄ cache_manager.rs         # Caching
‚îú‚îÄ‚îÄ data/                       # Vector store data
‚îú‚îÄ‚îÄ Cargo.toml                  # Rust dependencies
‚îú‚îÄ‚îÄ .env.example                # Environment template
‚îî‚îÄ‚îÄ README.md                   # This file
```

## üîå Integration with Next.js Frontend

The frontend connects to the backend via REST API:

```typescript
const response = await fetch('http://localhost:8000/api/search', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ query: 'your query', k: 5 })
});
```

## üêõ Debugging

Enable debug logging:

```bash
RUST_LOG=debug cargo run
```

## üìä Performance Notes

- Vector search uses cosine similarity with in-memory storage
- Embeddings are cached to reduce redundant computations
- Query responses are cached to minimize API calls
- Async I/O with Actix-web for high concurrency

## üöÄ Production Deployment

### Docker Build

```bash
docker build -t knora-backend .
docker run -p 8000:8000 --env-file .env knora-backend
```

### Binary Release

```bash
cargo build --release
./target/release/knora-backend
```

## üìù License

MIT License

## ü§ù Contributing

Contributions welcome! Please follow Rust conventions and add tests for new features.
```

Now let me create a Docker file for the backend:

<file_path>
knora_ai-main\knora-rust-nextjs\backend\Dockerfile
</file_path>

<edit_description>
Create Dockerfile for Rust backend
</edit_description>

```
# Build stage
FROM rust:1.75 as builder

WORKDIR /app

# Copy manifests
COPY Cargo.toml Cargo.toml

# Copy source
COPY src src

# Build the application
RUN cargo build --release

# Runtime stage
FROM debian:bookworm-slim

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copy binary from builder
COPY --from=builder /app/target/release/knora-backend /app/knora-backend

# Create data directory
RUN mkdir -p /app/data

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/api/health || exit 1

# Run the application
CMD ["./knora-backend"]
```

Now let me create the Next.js frontend files. First the main structure:

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\package.json
</file_path>

<edit_description>
Create Next.js package.json
</edit_description>

```json
{
  "name": "knora-frontend",
  "version": "2.0.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint",
    "type-check": "tsc --noEmit"
  },
  "dependencies": {
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "next": "^14.0.0",
    "axios": "^1.6.2",
    "zustand": "^4.4.0",
    "react-markdown": "^9.0.0",
    "framer-motion": "^10.16.0",
    "react-hot-toast": "^2.4.1",
    "lucide-react": "^0.302.0",
    "clsx": "^2.0.0",
    "tailwindcss": "^3.3.0",
    "typescript": "^5.3.2"
  },
  "devDependencies": {
    "@types/react": "^18.2.0",
    "@types/react-dom": "^18.2.0",
    "@types/node": "^20.0.0",
    "autoprefixer": "^10.4.16",
    "postcss": "^8.4.32",
    "eslint": "^8.54.0",
    "eslint-config-next": "^14.0.0"
  }
}
```

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\tsconfig.json
</file_path>

<edit_description>
Create TypeScript configuration
</edit_description>

```json
{
  "compilerOptions": {
    "target": "ES2020",
    "useDefineForClassFields": true,
    "lib": ["ES2020", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "skipLibCheck": true,
    "esModuleInterop": true,
    "allowSyntheticDefaultImports": true,
    "strict": true,
    "forceConsistentCasingInFileNames": true,
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx",
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "baseUrl": ".",
    "paths": {
      "@/*": ["./*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx"],
  "exclude": ["node_modules"]
}
```

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\next.config.js
</file_path>

<edit_description>
Create Next.js configuration
</edit_description>

```javascript
/** @type {import('next').NextConfig} */
const nextConfig = {
  reactStrictMode: true,
  swcMinify: true,
  env: {
    NEXT_PUBLIC_API_URL: process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8000/api',
  },
  headers: async () => {
    return [
      {
        source: '/:path*',
        headers: [
          {
            key: 'X-Content-Type-Options',
            value: 'nosniff',
          },
          {
            key: 'X-Frame-Options',
            value: 'SAMEORIGIN',
          },
        ],
      },
    ];
  },
};

module.exports = nextConfig;
```

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\tailwind.config.js
</file_path>

<edit_description>
Create Tailwind CSS configuration
</edit_description>

```javascript
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    './app/**/*.{js,ts,jsx,tsx,mdx}',
    './pages/**/*.{js,ts,jsx,tsx,mdx}',
    './components/**/*.{js,ts,jsx,tsx,mdx}',
  ],
  theme: {
    extend: {
      colors: {
        primary: '#3B82F6',
        secondary: '#8B5CF6',
        accent: '#EC4899',
      },
      animation: {
        'fade-in': 'fadeIn 0.5s ease-in',
        'slide-up': 'slideUp 0.5s ease-out',
      },
      keyframes: {
        fadeIn: {
          '0%': { opacity: '0' },
          '100%': { opacity: '1' },
        },
        slideUp: {
          '0%': { transform: 'translateY(10px)', opacity: '0' },
          '100%': { transform: 'translateY(0)', opacity: '1' },
        },
      },
    },
  },
  plugins: [],
};
```

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\postcss.config.js
</file_path>

<edit_description>
Create PostCSS configuration
</edit_description>

```javascript
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
};
```

Now create the Next.js app directory structure:

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\.env.example
</file_path>

<edit_description>
Create frontend environment example
</edit_description>

```
# API Configuration
NEXT_PUBLIC_API_URL=http://localhost:8000/api

# Development
NODE_ENV=development
```

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\app\layout.tsx
</file_path>

<edit_description>
Create root layout component
</edit_description>

```typescript
import type { Metadata } from 'next'
import './globals.css'
import { Toaster } from 'react-hot-toast'

export const metadata: Metadata = {
  title: 'KnoRa AI - Knowledge Assistant',
  description: 'Intelligent document analysis and question answering system',
}

export default function RootLayout({
  children,
}: {
  children: React.ReactNode
}) {
  return (
    <html lang="en">
      <body className="bg-gradient-to-br from-slate-900 to-slate-800 text-white">
        <Toaster position="top-right" />
        {children}
      </body>
    </html>
  )
}
```

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\app\globals.css
</file_path>

<edit_description>
Create global styles
</edit_description>

```css
@tailwind base;
@tailwind components;
@tailwind utilities;

* {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
}

html,
body {
  height: 100%;
  overflow-x: hidden;
}

body {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

/* Scrollbar styling */
::-webkit-scrollbar {
  width: 8px;
}

::-webkit-scrollbar-track {
  background: rgba(15, 23, 42, 0.5);
}

::-webkit-scrollbar-thumb {
  background: rgba(100, 116, 139, 0.7);
  border-radius: 4px;
}

::-webkit-scrollbar-thumb:hover {
  background: rgba(100, 116, 139, 0.9);
}

/* Animation utilities */
@keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}

@keyframes slideUp {
  from {
    transform: translateY(20px);
    opacity: 0;
  }
  to {
    transform: translateY(0);
    opacity: 1;
  }
}

.animate-fade-in {
  animation: fadeIn 0.5s ease-in;
}

.animate-slide-up {
  animation: slideUp 0.5s ease-out;
}
```

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\app\page.tsx
</file_path>

<edit_description>
Create main page component
</edit_description>

```typescript
'use client'

import { useEffect, useState } from 'react'
import DocumentIngestionTab from '@/components/DocumentIngestionTab'
import IntelligentQueryTab from '@/components/IntelligentQueryTab'
import KnowledgeAnalyticsTab from '@/components/KnowledgeAnalyticsTab'
import Navigation from '@/components/Navigation'
import { motion } from 'framer-motion'

type TabType = 'ingestion' | 'query' | 'analytics'

export default function Home() {
  const [activeTab, setActiveTab] = useState<TabType>('query')
  const [isLoading, setIsLoading] = useState(true)

  useEffect(() => {
    // Check backend health
    const checkHealth = async () => {
      try {
        const response = await fetch(`${process.env.NEXT_PUBLIC_API_URL}/health`)
        if (!response.ok) throw new Error('Backend unreachable')
      } catch (error) {
        console.error('Backend health check failed:', error)
      } finally {
        setIsLoading(false)
      }
    }

    checkHealth()
  }, [])

  const containerVariants = {
    hidden: { opacity: 0 },
    visible: {
      opacity: 1,
      transition: { staggerChildren: 0.1 },
    },
  }

  const itemVariants = {
    hidden: { opacity: 0, y: 20 },
    visible: { opacity: 1, y: 0 },
  }

  if (isLoading) {
    return (
      <div className="flex items-center justify-center min-h-screen">
        <div className="text-center">
          <div className="animate-spin rounded-full h-12 w-12 border-b-2 border-blue-500 mx-auto mb-4"></div>
          <p className="text-gray-400">Initializing KnoRa AI...</p>
        </div>
      </div>
    )
  }

  return (
    <motion.div
      className="min-h-screen"
      variants={containerVariants}
      initial="hidden"
      animate="visible"
    >
      <Navigation activeTab={activeTab} setActiveTab={setActiveTab} />

      <motion.main
        className="container mx-auto px-4 py-8"
        variants={itemVariants}
      >
        {activeTab === 'ingestion' && <DocumentIngestionTab />}
        {activeTab === 'query' && <IntelligentQueryTab />}
        {activeTab === 'analytics' && <KnowledgeAnalyticsTab />}
      </motion.main>
    </motion.div>
  )
}
```

Now let me create the utility API client:

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\lib\api.ts
</file_path>

<edit_description>
Create API client utility
</edit_description>

```typescript
import axios, { AxiosInstance } from 'axios'

const API_URL = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8000/api'

const apiClient: AxiosInstance = axios.create({
  baseURL: API_URL,
  headers: {
    'Content-Type': 'application/json',
  },
})

export interface ProcessFileRequest {
  file_path: string
  chunk_size?: number
  chunk_overlap?: number
}

export interface SearchRequest {
  query: string
  k?: number
  score_threshold?: number
}

export interface AnswerRequest {
  query: string
  retrieved_chunks: any[]
  max_tokens?: number
  temperature?: number
}

export const apiService = {
  // Health
  checkHealth: () => apiClient.get('/health'),

  // Documents
  processFile: (data: ProcessFileRequest) =>
    apiClient.post('/documents/process', data),
  getFileStats: (filePath: string) =>
    apiClient.get('/documents/stats', { params: { file_path: filePath } }),

  // Search
  search: (data: SearchRequest) => apiClient.post('/search', data),
  getVectorStoreStats: () => apiClient.get('/search/stats'),
  addDocuments: (documents: any[]) =>
    apiClient.post('/search/add', documents),
  deleteDocument: (filePath: string) =>
    apiClient.delete('/search/delete', { params: { file_path: filePath } }),
  clearStore: () => apiClient.delete('/search/clear'),

  // LLM
  generateAnswer: (data: AnswerRequest) =>
    apiClient.post('/llm/answer', data),
  getModelInfo: () => apiClient.get('/llm/model-info'),
  getSupportedModels: () => apiClient.get('/llm/models'),
}

export default apiClient
```

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\lib\store.ts
</file_path>

<edit_description>
Create Zustand state store
</edit_description>

```typescript
import { create } from 'zustand'

export interface Document {
  file_path: string
  file_name: string
  file_type: string
  num_chunks: number
  file_size: number
}

export interface SearchResult {
  file_name: string
  file_path: string
  chunk_id: number
  similarity_score: number
  text: string
}

interface AppStore {
  documents: Document[]
  setDocuments: (docs: Document[]) => void
  addDocument: (doc: Document) => void
  removeDocument: (filePath: string) => void
  clearDocuments: () => void

  searchResults: SearchResult[]
  setSearchResults: (results: SearchResult[]) => void
  clearSearchResults: () => void

  currentQuery: string
  setCurrentQuery: (query: string) => void

  isLoading: boolean
  setIsLoading: (loading: boolean) => void

  error: string | null
  setError: (error: string | null) => void
}

export const useStore = create<AppStore>((set) => ({
  documents: [],
  setDocuments: (docs) => set({ documents: docs }),
  addDocument: (doc) =>
    set((state) => ({
      documents: [...state.documents, doc],
    })),
  removeDocument: (filePath) =>
    set((state) => ({
      documents: state.documents.filter((d) => d.file_path !== filePath),
    })),
  clearDocuments: () => set({ documents: [] }),

  searchResults: [],
  setSearchResults: (results) => set({ searchResults: results }),
  clearSearchResults: () => set({ searchResults: [] }),

  currentQuery: '',
  setCurrentQuery: (query) => set({ currentQuery: query }),

  isLoading: false,
  setIsLoading: (loading) => set({ isLoading: loading }),

  error: null,
  setError: (error) => set({ error }),
}))
```

Now let me create the component directory with key components:

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\components\Navigation.tsx
</file_path>

<edit_description>
Create navigation component
</edit_description>

```typescript
'use client'

import { motion } from 'framer-motion'
import { FileText, Search, BarChart3 } from 'lucide-react'

interface NavigationProps {
  activeTab: 'ingestion' | 'query' | 'analytics'
  setActiveTab: (tab: 'ingestion' | 'query' | 'analytics') => void
}

const tabs = [
  { id: 'ingestion', label: 'üìÅ Document Ingestion', icon: FileText },
  { id: 'query', label: 'üîç Intelligent Query', icon: Search },
  { id: 'analytics', label: 'üìä Knowledge Analytics', icon: BarChart3 },
] as const

export default function Navigation({ activeTab, setActiveTab }: NavigationProps) {
  return (
    <nav className="bg-gradient-to-r from-blue-900 to-purple-900 shadow-lg sticky top-0 z-50">
      <div className="container mx-auto px-4">
        <div className="flex items-center justify-between h-16">
          <motion.div
            initial={{ opacity: 0, x: -20 }}
            animate={{ opacity: 1, x: 0 }}
            className="flex items-center space-x-2"
          >
            <div className="w-10 h-10 rounded-lg bg-gradient-to-br from-blue-400 to-purple-500 flex items-center justify-center">
              <span className="text-xl font-bold">üîç</span>
            </div>
            <h1 className="text-2xl font-bold">KnoRa AI</h1>
          </motion.div>

          <div className="flex space-x-2">
            {tabs.map((tab) => (
              <motion.button
                key={tab.id}
                onClick={() => setActiveTab(tab.id as any)}
                className={`px-4 py-2 rounded-lg font-medium transition-all ${
                  activeTab === tab.id
                    ? 'bg-blue-500 text-white shadow-lg'
                    : 'text-gray-300 hover:bg-blue-800/50'
                }`}
                whileHover={{ scale: 1.05 }}
                whileTap={{ scale: 0.95 }}
              >
                {tab.label}
              </motion.button>
            ))}
          </div>
        </div>
      </div>
    </nav>
  )
}
```

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\components\DocumentIngestionTab.tsx
</file_path>

<edit_description>
Create document ingestion tab component
</edit_description>

```typescript
'use client'

import { useState } from 'react'
import { motion } from 'framer-motion'
import toast from 'react-hot-toast'
import { apiService } from '@/lib/api'
import { useStore } from '@/lib/store'
import { Upload, Loader } from 'lucide-react'

export default function DocumentIngestionTab() {
  const { documents, addDocument, removeDocument } = useStore()
  const [filePath, setFilePath] = useState('')
  const [chunkSize, setChunkSize] = useState(1000)
  const [chunkOverlap, setChunkOverlap] = useState(200)
  const [isProcessing, setIsProcessing] = useState(false)

  const handleProcess = async (e: React.FormEvent) => {
    e.preventDefault()

    if (!filePath.trim()) {
      toast.error('Please enter a file path')
      return
    }

    setIsProcessing(true)
    try {
      const response = await apiService.processFile({
        file_path: filePath,
        chunk_size: chunkSize,
        chunk_overlap: chunkOverlap,
      })

      if (response.data.success && response.data.document) {
        addDocument(response.data.document)
        toast.success(`Document processed: ${response.data.document.file_name}`)
        setFilePath('')
      }
    } catch (error: any) {
      toast.error(error.response?.data?.message || 'Failed to process document')
    } finally {
      setIsProcessing(false)
    }
  }

  const handleDelete = async (filePath: string) => {
    try {
      await apiService.deleteDocument(filePath)
      removeDocument(filePath)
      toast.success('Document removed from vector store')
    } catch (error: any) {
      toast.error('Failed to delete document')
    }
  }

  return (
    <motion.div
      initial={{ opacity: 0, y: 20 }}
      animate={{ opacity: 1, y: 0 }}
      className="space-y-6"
    >
      <div className="bg-gradient-to-br from-blue-900/20 to-purple-900/20 rounded-lg p-6 border border-blue-500/20">
        <h2 className="text-2xl font-bold mb-4 flex items-center gap-2">
          <Upload size={28} />
          Document Processing
        </h2>

        <form onSubmit={handleProcess} className="space-y-4">
          <div>
            <label className="block text-sm font-medium mb-2">File Path</label>
            <input
              type="text"
              value={filePath}
              onChange={(e) => setFilePath(e.target.value)}
              placeholder="Enter full path to document..."
              className="w-full px-4 py-2 bg-slate-700 border border-slate-600 rounded-lg focus:outline-none focus:border-blue-500"
            />
          </div>

          <div className="grid grid-cols-2 gap-4">
            <div>
              <label className="block text-sm font-medium mb-2">Chunk Size</label>
              <input
                type="number"
                value={chunkSize}
                onChange={(e) => setChunkSize(Number(e.target.value))}
                min="100"
                max="5000"
                className="w-full px-4 py-2 bg-slate-700 border border-slate-600 rounded-lg focus:outline-none focus:border-blue-500"
              />
            </div>
            <div>
              <label className="block text-sm font-medium mb-2">Chunk Overlap</label>
              <input
                type="number"
                value={chunkOverlap}
                onChange={(e) => setChunkOverlap(Number(e.target.value))}
                min="0"
                max="500"
                className="w-full px-4 py-2 bg-slate-700 border border-slate-600 rounded-lg focus:outline-none focus:border-blue-500"
              />
            </div>
          </div>

          <button
            type="submit"
            disabled={isProcessing}
            className="w-full px-6 py-3 bg-gradient-to-r from-blue-500 to-purple-600 rounded-lg font-medium hover:shadow-lg transition-all disabled:opacity-50"
          >
            {isProcessing ? (
              <span className="flex items-center justify-center gap-2">
                <Loader size={20} className="animate-spin" />
                Processing...
              </span>
            ) : (
              'üöÄ Process File'
            )}
          </button>
        </form>
      </div>

      {documents.length > 0 && (
        <div className="space-y-3">
          <h3 className="text-lg font-semibold">Processed Documents</h3>
          {documents.map((doc) => (
            <motion.div
              key={doc.file_path}
              initial={{ opacity: 0, x: -20 }}
              animate={{ opacity: 1, x: 0 }}
              className="bg-slate-700/50 p-4 rounded-lg border border-slate-600"
            >
              <div className="flex justify-between items-center">
                <div>
                  <p className="font-medium">{doc.file_name}</p>
                  <p className="text-sm text-gray-400">
                    {doc.num_chunks} chunks ‚Ä¢ {(doc.file_size / 1024).toFixed(2)} KB
                  </p>
                </div>
                <button
                  onClick={() => handleDelete(doc.file_path)}
                  className="px-3 py-1 bg-red-600/20 text-red-400 rounded hover:bg-red-600/40 transition-all"
                >
                  Remove
                </button>
              </div>
            </motion.div>
          ))}
        </div>
      )}
    </motion.div>
  )
}
```

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\components\IntelligentQueryTab.tsx
</file_path>

<edit_description>
Create intelligent query tab component
</edit_description>

```typescript
'use client'

import { useState } from 'react'
import { motion } from 'framer-motion'
import toast from 'react-hot-toast'
import { apiService } from '@/lib/api'
import { useStore } from '@/lib/store'
import { Send, Loader } from 'lucide-react'
import ReactMarkdown from 'react-markdown'

export default function IntelligentQueryTab() {
  const { setIsLoading, isLoading } = useStore()
  const [query, setQuery] = useState('')
  const [numSources, setNumSources] = useState(5)
  const [similarity, setSimilarity] = useState(0.0)
  const [temperature, setTemperature] = useState(1.0)
  const [maxTokens, setMaxTokens] = useState(8192)
  const [answer, setAnswer] = useState<string | null>(null)
  const [sources, setSources] = useState<any[]>([])

  const handleSearch = async (e: React.FormEvent) => {
    e.preventDefault()

    if (!query.trim()) {
      toast.error('Please enter a query')
      return
    }

    setIsLoading(true)
    try {
      // First, search for relevant chunks
      const searchResponse = await apiService.search({
        query,
        k: numSources,
        score_threshold: similarity,
      })

      if (searchResponse.data.results.length === 0) {
        toast.error('No relevant documents found')
        setIsLoading(false)
        return
      }

      // Then, generate an answer using those chunks
      const answerResponse = await apiService.generateAnswer({
        query,
        retrieved_chunks: searchResponse.data.results,
        max_tokens: maxTokens,
        temperature,
      })

      setAnswer(answerResponse.data.answer)
      setSources(answerResponse.data.sources)
      toast.success('Answer generated successfully')
    } catch (error: any) {
      toast.error(error.response?.data?.error || 'Failed to generate answer')
    } finally {
      setIsLoading(false)
    }
  }

  return (
    <motion.div
      initial={{ opacity: 0, y: 20 }}
      animate={{ opacity: 1, y: 0 }}
      className="space-y-6"
    >
      <div className="bg-gradient-to-br from-blue-900/20 to-purple-900/20 rounded-lg p-6 border border-blue-500/20">
        <h2 className="text-2xl font-bold mb-4 flex items-center gap-2">
          üîç Ask Questions
        </h2>

        <form onSubmit={handleSearch} className="space-y-4">
          <div>
            <label className="block text-sm font-medium mb-2">Your Question</label>
            <textarea
              value={query}
              onChange={(e) => setQuery(e.target.value)}
              placeholder="Ask anything about your documents..."
              rows={4}
              className="w-full px-4 py-2 bg-slate-700 border border-slate-600 rounded-lg focus:outline-none focus:border-blue-500 resize-none"
            />
          </div>

          <div className="grid grid-cols-2 md:grid-cols-4 gap-4">
            <div>
              <label className="block text-sm font-medium mb-1">Sources</label>
              <input
                type="number"
                value={numSources}
                onChange={(e) => setNumSources(Number(e.target.value))}
                min="1"
                max="10"
                className="w-full px-3 py-2 bg-slate-700 border border-slate-600 rounded-lg text-sm"
              />
            </div>
            <div>
              <label className="block text-sm font-medium mb-1">Similarity</label>
              <input
                type="number"
                value={similarity}
                onChange={(e) => setSimilarity(Number(e.target.value))}
                min="0"
                max="1"
                step="0.1"
                className="w-full px-3 py-2 bg-slate-700 border border-slate-600 rounded-lg text-sm"
              />
            </div>
            <div>
              <label className="block text-sm font-medium mb-1">Temperature</label>
              <input
                type="number"
                value={temperature}
                onChange={(e) => setTemperature(Number(e.target.value))}
                min="0"
                max="2"
                step="0.1"
                className="w-full px-3 py-2 bg-slate-700 border border-slate-600 rounded-lg text-sm"
              />
            </div>
            <div>
              <label className="block text-sm font-medium mb-1">Max Tokens</label>
              <input
                type="number"
                value={maxTokens}
                onChange={(e) => setMaxTokens(Number(e.target.value))}
                min="100"
                max="32768"
                className="w-full px-3 py-2 bg-slate-700 border border-slate-600 rounded-lg text-sm"
              />
            </div>
          </div>

          <button
            type="submit"
            disabled={isLoading}
            className="w-full px-6 py-3 bg-gradient-to-r from-blue-500 to-purple-600 rounded-lg font-medium hover:shadow-lg transition-all disabled:opacity-50"
          >
            {isLoading ? (
              <span className="flex items-center justify-center gap-2">
                <Loader size={20} className="animate-spin" />
                Searching...
              </span>
            ) : (
              <span className="flex items-center justify-center gap-2">
                <Send size={20} />
                Search & Analyze
              </span>
            )}
          </button>
        </form>
      </div>

      {answer && (
        <motion.div
          initial={{ opacity: 0, y: 20 }}
          animate={{ opacity: 1, y: 0 }}
          className="space-y-4"
        >
          <div className="bg-slate-700/50 rounded-lg p-6 border border-blue-500/20">
            <h3 className="text-lg font-semibold mb-3">Answer</h3>
            <div className="prose prose-invert max-w-none">
              <ReactMarkdown>{answer}</ReactMarkdown>
            </div>
          </div>

          {sources.length > 0 && (
            <div className="bg-slate-700/50 rounded-lg p-6 border border-green-500/20">
              <h3 className="text-lg font-semibold mb-3">Sources</h3>
              <div className="space-y-2">
                {sources.map((source, idx) => (
                  <motion.div
                    key={idx}
                    initial={{ opacity: 0, x: -10 }}
                    animate={{ opacity: 1, x: 0 }}
                    className="bg-slate-800 p-3 rounded border border-slate-600"
                  >
                    <p className="text-sm font-medium">{source.file_name}</p>
                    <p className="text-xs text-gray-400">
                      Score: {(source.similarity_score * 100).toFixed(2)}%
                    </p>
                  </motion.div>
                ))}
              </div>
            </div>
          )}
        </motion.div>
      )}
    </motion.div>
  )
}
```

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\components\KnowledgeAnalyticsTab.tsx
</file_path>

<edit_description>
Create knowledge analytics tab component
</edit_description>

```typescript
'use client'

import { useEffect, useState } from 'react'
import { motion } from 'framer-motion'
import toast from 'react-hot-toast'
import { apiService } from '@/lib/api'
import { BarChart3, Loader } from 'lucide-react'

interface Stats {
  total_vectors: number
  total_documents: number
  embedding_model: string
  dimension: number
  documents: string[]
  storage_size_mb: number
}

export default function KnowledgeAnalyticsTab() {
  const [stats, setStats] = useState<Stats | null>(null)
  const [isLoading, setIsLoading] = useState(true)

  useEffect(() => {
    const fetchStats = async () => {
      try {
        const response = await apiService.getVectorStoreStats()
        setStats(response.data)
      } catch (error) {
        toast.error('Failed to fetch analytics')
      } finally {
        setIsLoading(false)
      }
    }

    fetchStats()
    const interval = setInterval(fetchStats, 30000) // Refresh every 30 seconds

    return () => clearInterval(interval)
  }, [])

  if (isLoading) {
    return (
      <div className="flex items-center justify-center h-96">
        <Loader size={32} className="animate-spin" />
      </div>
    )
  }

  if (!stats) {
    return (
      <div className="text-center text-gray-400">
        No analytics data available
      </div>
    )
  }

  const avgChunksPerDoc =
    stats.total_documents > 0
      ? (stats.total_vectors / stats.total_documents).toFixed(1)
      : 0

  return (
    <motion.div
      initial={{ opacity: 0, y: 20 }}
      animate={{ opacity: 1, y: 0 }}
      className="space-y-6"
    >
      <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-4">
        {[
          { label: 'Total Vectors', value: stats.total_vectors },
          { label: 'Total Documents', value: stats.total_documents },
          { label: 'Embedding Dimension', value: stats.dimension },
          { label: 'Storage (MB)', value: stats.storage_size_mb.toFixed(2) },
        ].map((item, idx) => (
          <motion.div
            key={idx}
            initial={{ opacity: 0, y: 10 }}
            animate={{ opacity: 1, y: 0 }}
            transition={{ delay: idx * 0.1 }}
            className="bg-gradient-to-br from-blue-900/20 to-purple-900/20 rounded-lg p-4 border border-blue-500/20"
          >
            <p className="text-gray-400 text-sm">{item.label}</p>
            <p className="text-3xl font-bold text-blue-400 mt-2">
              {item.value}
            </p>
          </motion.div>
        ))}
      </div>

      <div className="bg-gradient-to-br from-blue-900/20 to-purple-900/20 rounded-lg p-6 border border-blue-500/20">
        <h3 className="text-lg font-semibold mb-4 flex items-center gap-2">
          <BarChart3 size={24} />
          System Configuration
        </h3>

        <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
          <div>
            <p className="text-gray-400 text-sm">Embedding Model</p>
            <p className="text-xl font-medium text-green-400 mt-1">
              {stats.embedding_model}
            </p>
          </div>
          <div>
            <p className="text-gray-400 text-sm">Average Chunks per Document</p>
            <p className="text-xl font-medium text-green-400 mt-1">
              {avgChunksPerDoc}
            </p>
          </div>
        </div>
      </div>

      {stats.documents.length > 0 && (
        <div className="bg-gradient-to-br from-blue-900/20 to-purple-900/20 rounded-lg p-6 border border-blue-500/20">
          <h3 className="text-lg font-semibold mb-4">Indexed Documents</h3>
          <div className="space-y-2">
            {stats.documents.map((doc, idx) => (
              <motion.div
                key={idx}
                initial={{ opacity: 0, x: -10 }}
                animate={{ opacity: 1, x: 0 }}
                transition={{ delay: idx * 0.05 }}
                className="bg-slate-700/50 p-3 rounded border border-slate-600 text-sm break-all"
              >
                {doc}
              </motion.div>
            ))}
          </div>
        </div>
      )}
    </motion.div>
  )
}
```

Finally, let me create the README for the frontend and the docker-compose file:

<file_path>
knora_ai-main\knora-rust-nextjs\frontend\README.md
</file_path>

<edit_description>
Create frontend README
</edit_description>

```markdown
# KnoRa AI Frontend - Next.js Implementation

A modern, responsive web interface for the KnoRa AI Knowledge Assistant built with Next.js 14, React 18, and TypeScript.

## üöÄ Features

- **Document Management**: Upload and process documents for semantic search
- **Intelligent Query Interface**: Ask natural language questions about documents
- **Real-time Analytics**: Monitor vector store statistics and performance
- **Responsive Design**: Mobile-friendly interface with Tailwind CSS
- **State Management**: Zustand for efficient state management
- **API Integration**: Seamless communication with Rust backend

## üìã Prerequisites

- Node.js 18+ (download from https://nodejs.org)
- npm or yarn package manager
- Running Rust backend (http://localhost:8000)

## üîß Setup

### 1. Install Dependencies

```bash
cd frontend
npm install
```

### 2. Configure Environment

Copy `.env.example` to `.env.local`:

```bash
cp .env.example .env.local
```

Update `NEXT_PUBLIC_API_URL` if backend is not running on localhost:

```env
NEXT_PUBLIC_API_URL=http://localhost:8000/api
```

### 3. Run Development Server

```bash
npm run dev
```

Open http://localhost:3000 in your browser.

## üèóÔ∏è Build for Production

```bash
npm run build
npm run start
```

## üìÅ Project Structure

```
frontend/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx              # Root layout
‚îÇ   ‚îú‚îÄ‚îÄ page.tsx                # Main page
‚îÇ   ‚îî‚îÄ‚îÄ globals.css             # Global styles
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ Navigation.tsx           # Tab navigation
‚îÇ   ‚îú‚îÄ‚îÄ DocumentIngestionTab.tsx # Document upload
‚îÇ   ‚îú‚îÄ‚îÄ IntelligentQueryTab.tsx  # Query interface
‚îÇ   ‚îî‚îÄ‚îÄ KnowledgeAnalyticsTab.tsx # Analytics dashboard
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îú‚îÄ‚îÄ api.ts                  # API client
‚îÇ   ‚îî‚îÄ‚îÄ store.ts                # Zustand store
‚îú‚îÄ‚îÄ public/                     # Static assets
‚îú‚îÄ‚îÄ package.json                # Dependencies
‚îú‚îÄ‚îÄ tsconfig.json               # TypeScript config
‚îú‚îÄ‚îÄ tailwind.config.js          # Tailwind config
‚îú‚îÄ‚îÄ next.config.js              # Next.js config
‚îú‚îÄ‚îÄ postcss.config.js           # PostCSS config
‚îî‚îÄ‚îÄ README.md                   # This file
```

## üé® UI Components

### Navigation
- Tab-based navigation for different features
- Smooth transitions using Framer Motion

### Document Ingestion
- File path input with validation
- Configurable chunk size and overlap
- Document list with deletion capability

### Intelligent Query
- Query input with real-time processing
- Configurable search parameters
- Markdown-rendered answers with source attribution

### Knowledge Analytics
- Real-time statistics display
- System configuration overview
- List of indexed documents

## üîå API Integration

The frontend communicates with the Rust backend via REST API:

```typescript
// Example: Search for relevant documents
const response = await apiService.search({
  query: 'What is machine learning?',
  k: 5,
  score_threshold: 0.0
});
```

## üß™ Testing

```bash
# Type checking
npm run type-check

# Linting
npm run lint

# Build check
npm run build
```

## üöÄ Deployment

### Vercel (Recommended)

```bash
# Push to GitHub and connect to Vercel
# https://vercel.com/new
```

### Docker

```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package.json .
RUN npm install
COPY . .
RUN npm run build
EXPOSE 3000
CMD ["npm", "start"]
```

Build and run:

```bash
docker build -t knora-frontend .
docker run -p 3000:3000 -e NEXT_PUBLIC_API_URL=http://backend:8000/api knora-frontend
```

## üéØ Key Technologies

- **Next.js 14**: React framework with app router
- **TypeScript**: Type-safe development
- **Tailwind CSS**: Utility-first CSS framework
- **Framer Motion**: Smooth animations
- **Zustand**: Lightweight state management
- **Axios**: HTTP client
- **React Markdown**: Markdown rendering

## üìù License

MIT License

## ü§ù Contributing

Contributions welcome! Please follow Next.js conventions and add tests for new features.
```

<file_path>
knora_ai-main\knora-rust-nextjs\docker-compose.yml
</file_path>

<edit_description>
Create docker-compose file for full stack
</edit_description>

```yaml
version: '3.8'

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - GROQ_API_KEY=${GROQ_API_KEY}
      - VECTOR_STORE_PATH=/app/data/vector_store
      - SERVER_HOST=0.0.0.0
      - SERVER_PORT=8000
      - RUST_LOG=info
    volumes:
      - ./backend/data:/app/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - knora-network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://backend:8000/api
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - knora-network

networks:
  knora-network:
    driver: bridge
```

Finally, create the main project README:

<file_path>
knora_ai-main\knora-rust-nextjs\README.md
</file_path>

<edit_description>
Create main project README
</edit_description>

```markdown
# KnoRa AI - Rust Backend + Next.js Frontend

A production-ready Retrieval-Augmented Generation (RAG) system with a high-performance Rust backend and modern Next.js frontend.

## üéØ Project Overview

KnoRa AI is a complete rewrite of the Python-based knowledge assistant, now featuring:

- **100% Rust Backend**: High-performance async API with Actix-web
- **Modern Next.js Frontend**: React 18 with TypeScript and Tailwind CSS
- **Vector Search**: FAISS-based semantic search with embeddings
- **LLM Integration**: Groq API for intelligent document analysis
- **Full Docker Support**: Easy deployment with docker-compose

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Next.js Frontend (Port 3000)      ‚îÇ
‚îÇ  - React 18 + TypeScript            ‚îÇ
‚îÇ  - Tailwind CSS + Framer Motion     ‚îÇ
‚îÇ  - Zustand State Management         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ HTTP REST API
                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Rust Backend (Port 8000)          ‚îÇ
‚îÇ  - Actix-web Framework              ‚îÇ
‚îÇ  - Async/Await with Tokio           ‚îÇ
‚îÇ  - FAISS Vector Search              ‚îÇ
‚îÇ  - Groq LLM Integration             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÇ Project Structure

```
knora-rust-nextjs/
‚îú‚îÄ‚îÄ backend/                    # Rust backend
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.rs            # Application entry
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.rs          # Configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.rs          # Data models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ handlers/          # API endpoints
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/          # Business logic
‚îÇ   ‚îú‚îÄ‚îÄ Cargo.toml             # Rust dependencies
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile             # Docker image
‚îÇ   ‚îî‚îÄ‚îÄ .env.example           # Environment template
‚îî‚îÄ‚îÄ frontend/                   # Next.js frontend
    ‚îú‚îÄ‚îÄ app/                    # App directory
    ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx           # Main page
    ‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx         # Root layout
    ‚îÇ   ‚îî‚îÄ‚îÄ globals.css        # Global styles
    ‚îú‚îÄ‚îÄ components/            # React components
    ‚îú‚îÄ‚îÄ lib/                   # Utilities
    ‚îú‚îÄ‚îÄ package.json           # Dependencies
    ‚îú‚îÄ‚îÄ tailwind.config.js     # Tailwind config
    ‚îî‚îÄ‚îÄ .env.example           # Environment template
‚îú‚îÄ‚îÄ docker-compose.yml         # Full stack setup
‚îî‚îÄ‚îÄ README.md                  # This file
```

## üöÄ Quick Start

### Prerequisites

- Rust 1.70+
- Node.js 18+
- Docker & Docker Compose (optional)
- Groq API key (free at https://console.groq.com)

### Option 1: Local Development

#### 1. Backend Setup

```bash
cd backend

# Copy environment file
cp .env.example .env

# Add your Groq API key to .env
echo "GROQ_API_KEY=your_key_here" >> .env

# Run the backend
cargo run --release
```

Backend will be available at `http://localhost:8000`

#### 2. Frontend Setup

```bash
cd ../frontend

# Copy environment file
cp .env.example .env.local

# Install dependencies
npm install

# Run the frontend
npm run dev
```

Frontend will be available at `http://localhost:3000`

### Option 2: Docker Compose (Recommended)

```bash
# Set your Groq API key
export GROQ_API_KEY=your_key_here

# Start both services
docker-compose up

# Access:
# - Frontend: http://localhost:3000
# - Backend API: http://localhost:8000/api
```

## üìö API Documentation

### Base URL
```
http://localhost:8000/api
```

### Endpoints

#### Health Check
```
GET /health
Response: { "status": "healthy", "service": "KnoRa AI Backend", "version": "2.0.0" }
```

#### Process Document
```
POST /documents/process
Body: { "file_path": "path/to/doc.pdf", "chunk_size": 1000, "chunk_overlap": 200 }
Response: { "success": true, "document": {...} }
```

#### Search
```
POST /search
Body: { "query": "What is AI?", "k": 5, "score_threshold": 0.0 }
Response: { "results": [...], "query": "...", "count": 5 }
```

#### Generate Answer
```
POST /llm/answer
Body: { "query": "...", "retrieved_chunks": [...], "max_tokens": 8192, "temperature": 1.0 }
Response: { "answer": "...", "sources": [...], "num_sources": 5 }
```

#### Get Vector Store Stats
```
GET /search/stats
Response: { "total_vectors": 1000, "total_documents": 5, ... }
```

See detailed documentation in `backend/README.md` and `frontend/README.md`.

## üîß Configuration

### Backend (.env)

```env
GROQ_API_KEY=your_groq_api_key_here
VECTOR_STORE_PATH=data/vector_store
EMBEDDING_MODEL=all-MiniLM-L6-v2
SERVER_HOST=127.0.0.1
SERVER_PORT=8000
DEFAULT_LLM_MODEL=openai/gpt-oss-120b
RUST_LOG=info
```

### Frontend (.env.local)

```env
NEXT_PUBLIC_API_URL=http://localhost:8000/api
NODE_ENV=development
```

## üß™ Testing

### Backend Tests

```bash
cd backend
cargo test
cargo test -- --nocapture  # With output
```

### Frontend Type Checking

```bash
cd frontend
npm run type-check
npm run lint
npm run build
```

## üìä Performance

- **Backend**: Async Rust with Actix-web for high concurrency
- **Vector Search**: In-memory FAISS with cosine similarity
- **Caching**: Embedding and query response caching
- **Frontend**: Next.js with static generation and incremental updates

## üöÄ Production Deployment

### Docker Compose

```bash
export GROQ_API_KEY=your_key
docker-compose up -d
```

### Kubernetes (Coming Soon)

Example manifests for EKS, GKE, or AKS deployment.

### Environment Variables for Production

```env
# Backend
GROQ_API_KEY=prod_key
SERVER_HOST=0.0.0.0
RUST_LOG=warn

# Frontend
NEXT_PUBLIC_API_URL=https://api.yourdomain.com
NODE_ENV=production
```

## üîí Security Notes

- Never commit API keys to version control
- Use `.env` files for local development
- Use environment variables or secrets management for production
- CORS is configured to accept any origin (configure for production)
- API keys should be stored securely

## üêõ Troubleshooting

### Backend won't start
- Check GROQ_API_KEY is set
- Verify port 8000 is available
- Check logs: `RUST_LOG=debug cargo run`

### Frontend connection failed
- Ensure backend is running on port 8000
- Check NEXT_PUBLIC_API_URL in .env.local
- Check browser console for CORS errors

### Vector store issues
- Ensure `data/vector_store` directory is writable
- Check disk space
- Clear store if corrupted: delete the directory

## üìù License

MIT License - See LICENSE file

## ü§ù Contributing

Contributions welcome! Please:

1. Follow Rust and TypeScript conventions
2. Add tests for new features
3. Update documentation
4. Submit pull requests

## üìû Support

- üìñ [Backend Documentation](./backend/README.md)
- üìñ [Frontend Documentation](./frontend/README.md)
- üêõ [Issue Tracker](https://github.com/yourusername/knora-ai)

## üöÄ Roadmap

- [ ] Kubernetes deployment manifests
- [ ] Advanced embedding models (BERT, E5)
- [ ] Multi-user support with authentication
- [ ] Document versioning
- [ ] Advanced analytics dashboard
- [ ] WebSocket support for real-time updates
- [ ] PDF, image, and table extraction improvements

---

Built with ‚ù§Ô∏è using Rust and React
```

Perfect! Now let me create a summary document:

<file_path>
knora_ai-main\knora-rust-nextjs\CONVERSION_SUMMARY.md
</file_path>

<edit_description>
Create conversion summary document
</edit_description>

```markdown
# Python to Rust + Next.js Conversion Summary

## Overview

The KnoRa AI Knowledge Assistant has been successfully converted from a Python-based Streamlit application to a production-ready architecture with:
- **100% Rust Backend**: High-performance async REST API
- **Modern Next.js Frontend**: React 18 with TypeScript

## What Was Converted

### Python Services ‚Üí Rust Modules

1. **document_processor.py** ‚Üí `src/services/document_processor.rs`
   - File parsing and text extraction
   - Smart chunking algorithm with overlap
   - Support for multiple file formats

2. **vector_store.py** ‚Üí `src/services/vector_store.rs`
   - FAISS integration for vector search
   - Embedding generation and caching
   - Document metadata management
   - Similarity search implementation

3. **llm_handler.py** ‚Üí `src/services/llm_handler.rs`
   - Groq API integration
   - Response generation with streaming
   - Query response caching

4. **cache_manager.py** ‚Üí `src/services/cache_manager.rs`
   - Embedding cache with LRU eviction
   - Query response cache
   - Cache statistics tracking

5. **config settings** ‚Üí `src/config.rs`
   - Environment variable management
   - Application configuration
   - Embedding model constants

### Streamlit UI ‚Üí Next.js Frontend

1. **Main
